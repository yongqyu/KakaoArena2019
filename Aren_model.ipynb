{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96892 310758 19065\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "item_dict = np.load('/data/private/Arena/prepro_results/item_dict.npy', allow_pickle=True).item()\n",
    "item_list = np.load('/data/private/Arena/prepro_results/item_list.npy')\n",
    "keyword_dict = np.load('/data/private/Arena/prepro_results/keyword_dict.npy', allow_pickle=True).item()\n",
    "keyword_list = np.load('/data/private/Arena/prepro_results/keyword_list.npy')\n",
    "id2reader = np.load('/data/private/Arena/prepro_results/id2reader.npy')\n",
    "reader2id = np.load('/data/private/Arena/prepro_results/reader2id.npy', allow_pickle=True).item()\n",
    "id2writer = np.load('/data/private/Arena/prepro_results/id2writer.npy')\n",
    "writer2id = np.load('/data/private/Arena/prepro_results/writer2id.npy', allow_pickle=True).item()\n",
    "item2keywd = np.load('/data/private/Arena/prepro_results/item2keywd.npy', allow_pickle=True).item()\n",
    "keyword_dict['없음'] = len(keyword_list)\n",
    "keyword_list = list(keyword_list)\n",
    "keyword_list.append('없음')\n",
    "\n",
    "num_keywords = len(keyword_dict)\n",
    "num_readers = len(id2reader)\n",
    "num_writers = len(id2writer)\n",
    "print(num_keywords, num_readers, num_writers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_metrics as metrics\n",
    "\n",
    "class GMF(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_keywd, latent_dim):\n",
    "        super(GMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_keywd = num_keywd\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=2 * self.latent_dim)\n",
    "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
    "        self.embedding_keywd = torch.nn.Embedding(num_embeddings=self.num_keywd, embedding_dim=self.latent_dim)\n",
    "\n",
    "        self.item4valid = None\n",
    "        \n",
    "        self.affine_output = torch.nn.Linear(in_features=self.latent_dim, out_features=1)\n",
    "        self.logistic = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices, item_keywd, negs_indices, negs_keywd):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        negs_embedding = self.embedding_item(negs_indices)\n",
    "        item_kw_embedding = self.embedding_keywd(item_keywd)\n",
    "        negs_kw_embedding = self.embedding_keywd(negs_keywd)\n",
    "        \n",
    "        element_product = torch.mul(user_embedding, torch.cat([item_embedding, item_kw_embedding], 1))\n",
    "        pos_logits = self.affine_output(element_product)\n",
    "        \n",
    "        element_product = torch.mul(user_embedding, torch.cat([negs_embedding, negs_kw_embedding], 1))\n",
    "        neg_logits = self.affine_output(element_product)\n",
    "        loss = - self.logistic(pos_logits) + self.logistic(neg_logits)\n",
    "        \n",
    "        return torch.sum(loss)\n",
    "    \n",
    "    def predict(self, user_indices, top=100):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(self.item4valid[:,0])\n",
    "        keywd_embedding = self.embedding_keywd(self.item4valid[:,1])\n",
    "        \n",
    "        element_product = torch.mul(user_embedding, torch.cat([item_embedding, keywd_embedding], 1))\n",
    "        logits = self.affine_output(element_product)\n",
    "        print(logits.size())\n",
    "        _, sorted_indices = torch.sort(logits, 0, descending=True)\n",
    "        sorted_indices = sorted_indices.squeeze()[:100].cpu().numpy().tolist()\n",
    "        \n",
    "        return sorted_indices\n",
    "\n",
    "    def set_item4valid(self, item4valid):\n",
    "        self.item4valid = item4valid\n",
    "        \n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "read_path = '/data/private/Arena/datasets/read/'\n",
    "read_files = os.listdir(read_path)\n",
    "train_read_files = read_files[:int(len(read_files)*0.8)]\n",
    "valid_read_files = read_files[int(len(read_files)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a142fc1ae0141d4902b0cfe7d2a931e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2900), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_subs_keywd = []; train_subs = []; train_negs_keywd = []; train_negs = []; train_reader = []\n",
    "for read_file in tqdm.tqdm_notebook(train_read_files):\n",
    "    file = open(read_path+read_file, 'r')\n",
    "    data = file.readlines()\n",
    "    for line in data:\n",
    "        line = line.split(' ')\n",
    "        try:\n",
    "            train_reader += [reader2id[line[0]]]*len(line[1:-1])\n",
    "            trian_subs_keywd += [keyword_dict[item2keywd[x][0] if item2keywd[x][0] is True else '없음'] for x in line[1:-1]]\n",
    "            train_subs += [writer2id[x.split('_')[0]] for x in line[1:-1]]\n",
    "            train_negs_keywd += random.sample(list(set(range(num_keywords))-set(subs_keywd)), len(subs_keywd))\n",
    "            train_negs += random.sample(list(set(range(num_writers))-set(subs)), len(subs))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "train_subs_keywd = torch.from_numpy(np.array(train_subs_keywd)).cuda()\n",
    "train_subs = torch.from_numpy(np.array(train_subs)).cuda()\n",
    "train_negs_keywd = torch.from_numpy(np.array(train_negs_keywd)).cuda()\n",
    "train_negs = torch.from_numpy(np.array(train_negs)).cuda()\n",
    "train_reader = torch.from_numpy(np.array(train_reader)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811426339d014061b08c649c67cfb763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=726), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_subs = []; valid_reader = []\n",
    "for read_file in tqdm.tqdm_notebook(valid_read_files):\n",
    "    file = open(read_path+read_file, 'r')\n",
    "    data = file.readlines()\n",
    "    for line in data:\n",
    "        line = line.split(' ')\n",
    "        try:\n",
    "            valid_subs += [item_dict[x] for x in line[1:-1]]\n",
    "            valid_reader += [reader2id[line[0]]]*len(valid_subs)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "valid_subs = torch.from_numpy(np.array(valid_subs)).cuda()\n",
    "valid_reader = torch.from_numpy(np.array(valid_reader)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "hidden_dim = 128\n",
    "val_step = 1\n",
    "batch_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMF(\n",
      "  (embedding_user): Embedding(310758, 256)\n",
      "  (embedding_item): Embedding(19065, 128)\n",
      "  (embedding_keywd): Embedding(96892, 128)\n",
      "  (affine_output): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "# of params :  94396673\n"
     ]
    }
   ],
   "source": [
    "model = GMF(num_readers, num_writers, num_keywords, hidden_dim).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "print(model)\n",
    "print('# of params : ', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([643104, 2])\n"
     ]
    }
   ],
   "source": [
    "item4valid = []\n",
    "for i, item in enumerate(item_list):\n",
    "    \n",
    "    writer = writer2id[item.split('_')[0]]\n",
    "    keyword = keyword_dict[item2keywd[item][0]]\n",
    "    item4valid.append([writer, keyword])\n",
    "item4valid = torch.from_numpy(np.array(item4valid)).cuda()\n",
    "print(item4valid.size())\n",
    "model.set_item4valid(item4valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042eabf798794a7bb9772be6eeb0d8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2900), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for read_file in tqdm.tqdm_notebook(train_read_files):\n",
    "        file = open(read_path+read_file, 'r')\n",
    "        data = file.readlines()\n",
    "        for line in data:\n",
    "            line = line.split(' ')\n",
    "            try:\n",
    "                subs_keywd += [keyword_dict[item2keywd[x][0] if item2keywd[x][0] is True else '없음'] for x in line[1:-1]]\n",
    "                subs += [writer2id[x.split('_')[0]] for x in line[1:-1]]\n",
    "                negs_keywd += random.sample(list(set(range(num_keywords))-set(subs_keywd)), len(subs_keywd))\n",
    "                negs += random.sample(list(set(range(num_writers))-set(subs)), len(subs))\n",
    "                reader += [reader2id[line[0]]]*len(sub)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            if len(reader) < batch_size:\n",
    "                continue\n",
    "                \n",
    "            reader = torch.from_numpy(np.array([reader]*len(subs))).cuda()\n",
    "            subs_keywd = torch.from_numpy(np.array(subs_keywd)).cuda()\n",
    "            subs = torch.from_numpy(np.array(subs)).cuda()\n",
    "            negs_keywd = torch.from_numpy(np.array(negs_keywd)).cuda()\n",
    "            negs = torch.from_numpy(np.array(negs)).cuda()\n",
    "            try:\n",
    "                loss = model(reader, subs, subs_keywd, negs, negs_keywd)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            subs_keywd = []; subs = []; negs_keywd = []; negs = []; reader = []\n",
    "            \n",
    "    reader = torch.from_numpy(np.array([reader]*len(subs))).cuda()\n",
    "    subs_keywd = torch.from_numpy(np.array(subs_keywd)).cuda()\n",
    "    subs = torch.from_numpy(np.array(subs)).cuda()\n",
    "    negs_keywd = torch.from_numpy(np.array(negs_keywd)).cuda()\n",
    "    negs = torch.from_numpy(np.array(negs)).cuda()\n",
    "    try:\n",
    "        loss = model(reader, subs, subs_keywd, negs, negs_keywd)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1)%val_step == 0:\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0\n",
    "            model.eval()\n",
    "            i = 0\n",
    "            for read_file in valid_read_files[:2]:\n",
    "                file = open(read_path+read_file, 'r')\n",
    "                data = file.readlines()\n",
    "                for line in data:\n",
    "                    line = line.split(' ')\n",
    "                    try:\n",
    "                        reader = torch.from_numpy(np.array([reader2id[line[0]]])).cuda()\n",
    "                        subs = [item_dict[writer] for writer in line[1:-1]]\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                    preds = model.predict(reader)\n",
    "                    loss = metrics.apk(subs, preds, 100)                    \n",
    "                    valid_loss += loss\n",
    "                    i += 1\n",
    "                \n",
    "            print('epoch: '+str(epoch+1)+' MAP: '+str(valid_loss/(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_read_files), len(valid_read_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
