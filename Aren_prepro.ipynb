{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook # tqdm in .py\n",
    "\n",
    "dataset_path = '/data/private/Arena/datasets/'\n",
    "prepro_path = '/data/private/Arena/prepro_results/'\n",
    "\n",
    "magazine_path = dataset_path+'magazine.json'\n",
    "metadata_path = dataset_path+'metadata.json'\n",
    "users_path = dataset_path+'users.json'\n",
    "predict_path = dataset_path+'predict/'\n",
    "read_path = dataset_path+'read/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magazine -> keyword list\n",
    "magazine_list = []\n",
    "for line in open(magazine_path, 'r', encoding='utf-8'):\n",
    "    magazine_list.append(json.loads(line))\n",
    "metadata_list = []\n",
    "for line in open(metadata_path, 'r', encoding='utf-8'):\n",
    "    metadata_list.append(json.loads(line))\n",
    "users_list = []\n",
    "for line in open(users_path, 'r', encoding='utf-8'):\n",
    "    users_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magazine_list[0], metadata_list[0], users_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword / reader / writer dict and list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = ['unk', 'pad']\n",
    "for data in tqdm_notebook(magazine_list):\n",
    "    for keyword in data['magazine_tag_list']:\n",
    "        if keyword not in keyword_list:\n",
    "            keyword_list.append(keyword)\n",
    "for data in tqdm_notebook(metadata_list):\n",
    "    for keyword in data['keyword_list']:\n",
    "        if keyword not in keyword_list:\n",
    "            keyword_list.append(keyword)\n",
    "for data in tqdm_notebook(users_list):\n",
    "    for keyword in data['keyword_list']:\n",
    "        if keyword not in keyword_list:\n",
    "            keyword_list.append(keyword)\n",
    "            \n",
    "keyword_dict = {}\n",
    "for i, keyword in enumerate(keyword_list):\n",
    "    keyword_dict[keyword] = i\n",
    "    \n",
    "np.save(prepro_path+'keyword_dict.npy', keyword_dict)\n",
    "np.save(prepro_path+'keyword_list.npy', keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2writer = ['unk']\n",
    "id2reader = ['unk']\n",
    "for data in tqdm_notebook(users_list):\n",
    "    id2reader.append(data['id'])\n",
    "    for writer in data['following_list']:\n",
    "        if writer not in id2writer:\n",
    "            id2writer.append(writer)\n",
    "\n",
    "reader2id = {}\n",
    "for i, reader in enumerate(id2reader):\n",
    "    reader2id[reader] = i\n",
    "    \n",
    "writer2id = {}\n",
    "for i, writer in enumerate(id2writer):\n",
    "    writer2id[writer] = i\n",
    "    \n",
    "np.save(prepro_path+'id2reader.npy', id2reader)\n",
    "np.save(prepro_path+'reader2id.npy', reader2id)\n",
    "np.save(prepro_path+'id2writer.npy', id2writer)\n",
    "np.save(prepro_path+'writer2id.npy', writer2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magazine2id = {'unk':0}\n",
    "id2magazine = ['unk']\n",
    "for data in tqdm_notebook(magazine_list):\n",
    "    id_ = int(data['id'])\n",
    "    \n",
    "    if id_ not in id2magazine:\n",
    "        magazine2id[id_] = len(id2magazine)\n",
    "        id2magazine.append(id_)\n",
    "        \n",
    "np.save(prepro_path+'id2magazine.npy', id2magazine)\n",
    "np.save(prepro_path+'magazine2id.npy', magazine2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reader / item 2 elements & item dict and list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader2elem = {'unk':0}\n",
    "follow_list = []\n",
    "follow_maxlen = 8\n",
    "keyword_maxlen = 8\n",
    "for data in tqdm_notebook(users_list):\n",
    "    #follow_list.append(len(data['following_list']))\n",
    "    id_ = reader2id[data['id']]\n",
    "    follow = [writer2id['unk']] * (follow_maxlen-len(data['following_list'])) + \\\n",
    "             list(map(writer2id.get, data['following_list'][-follow_maxlen:]))\n",
    "\n",
    "    keywords = []\n",
    "    if data['keyword_list']:\n",
    "        keywords = [kw['keyword'].split(' ') for kw in data['keyword_list']]\n",
    "        keywords = list(set([a for b in keywords for a in b]))\n",
    "        keywords = [keyword_dict[kw] for kw in keywords if keyword_dict.get(kw) != None]\n",
    "        \n",
    "    keywords = [keyword_dict['pad']] * (keyword_maxlen-len(keywords)) + keywords[:keyword_maxlen]\n",
    "    reader2elem[id_] = follow + keywords\n",
    "\n",
    "np.save(prepro_path+'reader2elem.npy', reader2elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_min = 1538319600 # 20181001000000 GMT+8\n",
    "ts_max = 1552575600 # 20190315000000 GMT+8\n",
    "ts_gap = ts_max - ts_min\n",
    "\n",
    "keywd_maxlen = 5\n",
    "item2elem = {0:[0,0,0,0,0,0,0,0]}\n",
    "item_list = ['unk']\n",
    "item_dict = {'unk':0}\n",
    "\n",
    "for data in tqdm_notebook(metadata_list):\n",
    "\n",
    "    if item_dict.get(data['id']) == None:\n",
    "        item_dict[data['id']] = len(item_list)\n",
    "        item_list.append(data['id'])\n",
    "        \n",
    "    if data['keyword_list']:\n",
    "        keywd = [keyword_dict['pad']] * (keywd_maxlen-len(data['keyword_list'])) + \\\n",
    "                list(map(keyword_dict.get, data['keyword_list'][::-1]))\n",
    "    else:\n",
    "        keywd = [keyword_dict['unk']] * keywd_maxlen\n",
    "    writer = writer2id[data['user_id']]\n",
    "    reg_ts = int(data['reg_ts'])/1000\n",
    "    reg_ts = (reg_ts-ts_min)/ts_gap if reg_ts > ts_min else 0\n",
    "\n",
    "    if magazine2id.get(int(data['magazine_id'])) == None:\n",
    "        magazine2id[int(data['magazine_id'])] = len(id2magazine)\n",
    "        id2magazine.append(int(data['magazine_id']))\n",
    "    mag_id = magazine2id[int(data['magazine_id'])]\n",
    "    \n",
    "    item2elem[item_dict[data['id']]] = [writer] + keywd + [reg_ts, mag_id]\n",
    "\n",
    "np.save(prepro_path+'item_dict.npy', item_dict)\n",
    "np.save(prepro_path+'item_list.npy', item_list)\n",
    "np.save(prepro_path+'item2elem.npy', item2elem)\n",
    "np.save(prepro_path+'id2magazine.npy', id2magazine)\n",
    "np.save(prepro_path+'magazine2id.npy', magazine2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid Tensor & Writer 2 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "\n",
    "valid_writer_keywd = [[0,0,0,0,0,0,0,0,0]]\n",
    "\n",
    "for data in tqdm_notebook(metadata_list):\n",
    "    if data['keyword_list']:\n",
    "        keywd = [keyword_dict['pad']] * (keywd_maxlen-len(data['keyword_list'])) + \\\n",
    "                list(map(keyword_dict.get, data['keyword_list'][::-1]))\n",
    "    else:\n",
    "        keywd = [keyword_dict['unk']] * keywd_maxlen\n",
    "    writer = writer2id[data['user_id']]\n",
    "    reg_ts = int(data['reg_ts'])/1000\n",
    "    reg_ts = (reg_ts-ts_min)/ts_gap if reg_ts > ts_min else 0\n",
    "    if magazine2id.get(int(data['magazine_id'])) == None:\n",
    "        magazine2id[int(data['magazine_id'])] = len(id2magazine)\n",
    "        id2magazine.append(int(data['magazine_id']))\n",
    "    mag_id = magazine2id[int(data['magazine_id'])]\n",
    "    item_id = item_dict[data['id']]\n",
    "    \n",
    "    valid_writer_keywd.append([item_id, writer] + keywd + [reg_ts, mag_id])\n",
    "\n",
    "valid_writer_keywd = torch.from_numpy(np.array(valid_writer_keywd))\n",
    "torch.save(valid_writer_keywd, prepro_path+'valid_writer_keywd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writerid2items = {}\n",
    "for data in tqdm_notebook(metadata_list):\n",
    "    user_id = writer2id[data['user_id']]\n",
    "    id_ = item_dict[data['id']]\n",
    "    keyword = keyword_dict[data['keyword_list'][0] if data['keyword_list'] is True else '없음']\n",
    "    \n",
    "    if writerid2items.get(user_id) == None:\n",
    "        writerid2items[user_id] = [[id_, user_id, keyword]]\n",
    "    else:\n",
    "        writerid2items[user_id].append([id_, user_id, keyword])\n",
    "        \n",
    "np.save(prepro_path+'writerid2items.npy', writerid2items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From 2019022 : Reader 2 Read item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "reader2item = {}\n",
    "file_list = os.listdir(read_path)\n",
    "file_list.sort()\n",
    "\n",
    "for read_file in tqdm_notebook(file_list[3456:]): #2월~(2952) #2월22~(3456) #2월20~(3408)\n",
    "    try:\n",
    "        file = open(read_path+read_file, 'r')\n",
    "        data_ = file.readlines()\n",
    "    except:\n",
    "        print(read_file)\n",
    "        continue\n",
    "        \n",
    "    file_ts = time.mktime(datetime.datetime.strptime(read_file[-10:], '%Y%m%d%H').timetuple()) + 32400\n",
    "    file_ts = (file_ts-ts_min)/ts_gap if file_ts > ts_min else 0\n",
    "    if file_ts < 0:\n",
    "        print('xx')\n",
    "\n",
    "    for line in data_:\n",
    "        tokens = line.split(' ')\n",
    "        try:\n",
    "            reader = reader2id[tokens[0]]\n",
    "        except:\n",
    "            continue\n",
    "        items = [[item_dict[x], file_ts] if item_dict.get(x)!=None else [item_dict['unk'], file_ts] for x in tokens[1:-1]]\n",
    "        \n",
    "        if reader2item.get(reader) != None:\n",
    "            reader2item[reader] = reader2item[reader] + items\n",
    "        else:\n",
    "            reader2item[reader] = items\n",
    "            \n",
    "np.save(prepro_path+'reader2item.npy', reader2item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid2followid = {}\n",
    "for data in users_list:\n",
    "    id_ = reader2id[data['id']]\n",
    "    following_list = [writer2id[x] for x in data['following_list']]\n",
    "    userid2followid[id_] = following_list\n",
    "    \n",
    "np.save(prepro_path+'userid2followid.npy', userid2followid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Based\n",
    "Train Valid Test data (from 4 to 1 : window size = 5) and Mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_train_data = []\n",
    "rnn_valid_data = []\n",
    "rnn_test_data = {}\n",
    "window_size = 5\n",
    "for reader, items_list in reader2item.items():\n",
    "    if not items_list:\n",
    "        continue\n",
    "    items_array = np.array(items_list)\n",
    "    items, read_ts = items_array[:,0].tolist(), items_array[:,1].tolist()\n",
    "\n",
    "    if len(items) < window_size:\n",
    "        items = [item_dict['unk']] * (window_size-len(items)) + items\n",
    "        read_ts = [0] * (window_size-len(read_ts)) + read_ts\n",
    "        \n",
    "    rnn_data = []\n",
    "    for i in range(len(items)-window_size+1):\n",
    "        rnn_data.append([reader, read_ts[i+window_size-1]] + items[i:i+window_size])\n",
    "        \n",
    "    if len(items) > window_size+4:\n",
    "        rnn_train_data += rnn_data[int(len(rnn_data)*0.1):]\n",
    "        rnn_valid_data += rnn_data[:int(len(rnn_data)*0.1)]\n",
    "    else:\n",
    "        rnn_train_data += rnn_data\n",
    "    rnn_test_data[reader] = rnn_data[-1]\n",
    "        \n",
    "np.save(prepro_path+'rnn_train_data.npy', rnn_train_data)\n",
    "np.save(prepro_path+'rnn_valid_data.npy', rnn_valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data as not a dictionary (REAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev_file = predict_path+'dev.users'\n",
    "pred_dev_data = open(pred_dev_file, 'r').readlines()\n",
    "test_data = []\n",
    "a = 0\n",
    "for line in pred_dev_data:\n",
    "    if reader2id.get(line.strip()) != None:\n",
    "        reader = reader2id[line.strip()]\n",
    "        readed = rnn_test_data[reader] if rnn_test_data.get(reader)!=None else [0] * (window_size+2)\n",
    "    else:\n",
    "        reader = reader2id['unk']\n",
    "        readed = [0] * (window_size+2)\n",
    "        a += 1\n",
    "    test_data.append(readed)\n",
    "\n",
    "np.save(prepro_path+'rnn_test_data.npy', np.array(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev_file = predict_path+'dev.users'\n",
    "pred_dev_data = open(pred_dev_file, 'r').readlines()\n",
    "\n",
    "dev_mask = np.ones((len(pred_dev_data), 643105))\n",
    "for i, line in enumerate(pred_dev_data):\n",
    "    try:\n",
    "        readed = reader2items[reader2id[line.strip()]]\n",
    "        readed = list(set(np.array(readed)[:,0].astype(np.int32).tolist()))\n",
    "    except:\n",
    "        continue\n",
    "    dev_mask[i,readed] = 0\n",
    "dev_mask[:,0] = 0\n",
    "    \n",
    "np.save(prepro_path+'dev_mask.npy', dev_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
